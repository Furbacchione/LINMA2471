%\documentclass[a4paper]{article}
%\usepackage[utf8]{inputenc}
%\usepackage{amsthm}             % Definitions and such
%\usepackage{amssymb}            % R for space and such
%\usepackage{amsmath}
%\usepackage[a4paper]{geometry}
%
%\title{LINMA2471 : Optimization models and models : course 6 (28/10/2015)}
%\author{Renaud Dufays, Antoine Durviaux \& LeÃ¯la Van Keirsbilck}
%\date{Octobre 2015}
%
%\usepackage{natbib}
%\usepackage{graphicx}
%\usepackage{framed}
%\usepackage{tikz}
%\usepackage{float}
%\usetikzlibrary{arrows}
%\usetikzlibrary{decorations.markings}
%
%
%\begin{document}
%
%\maketitle

\subsection{Gradient mapping for constrained problems}

\begin{definition}
For some $M>0$, the \textbf{gradient mapping} $G_M^Cf(x)$ is the unique vector satisfying 
\begin{equation*}
x - \frac{1}{M}G_M^Cf(x) = P_C\left[x - \frac{1}{M}\nabla f\right]
\end{equation*}
\end{definition}

The role of the gradient mapping is similar to the one of the gradient in the non-constraint case. Notice that if we take $C = \mathbb{R}^n$, then $G_M^{\mathbb{R}^n}f(x) = \nabla f(x)$. An illustration of the gradient mapping is given in Figure \ref{tik2}.

\begin{figure}[H]
\centering
	\begin{tikzpicture}
    \draw (0,0) circle (1);
    \draw (0,-1.3) node{$C$};
    \draw[>=latex,->, blue] (0.5,-0.5) -- (1.5,0);
    \draw[blue] (1.3,-0.5) node{$-\nabla f$};
    \draw[red] (0.4,0.1) node{$-G_M^C f$};
    \draw[>=latex,->,red] (0.5,-0.5) -- (1,0);
    \draw[dashed] (1,0) -- (1.5,0);
    \draw (4,0) circle (1);
    \draw (4,-1.3) node{$C$};
    \draw[>=latex,->, blue, dash pattern= on 3pt off 5pt] (3.5,-0.2) -- (3.5,0.5);
		\draw[>=latex,->, red, dash pattern= on 3pt off 5pt,dash phase=4pt] (3.5,-0.2) -- (3.5,0.5);
    \draw[blue] (4.4,0.2) node{$-\nabla f$};
		\draw[red] (5.7,0.2) node{$-G_M^C f$};
		\draw (5,0.2) node{$=$};
    \end{tikzpicture}
\caption{Illustration of gradient mapping.}
\label {tik2}
\end{figure}

% \draw[blue,dash pattern= on 3pt off 5pt] (0,0) |- (1,1) to[out=0,in=90] (2,0);
% \draw[red,dash pattern= on 3pt off 5pt,dash phase=4pt] (0,0) |- (1,1) to[out=0,in=90] (2,0);

The gradient method becomes, in the case of constraint problems :

\begin{lstlisting}[mathescape,caption=Gradient Method - Constrained Problem]
Given $x_0$, $L$, $k=0$
Repeat
$\qquad   x_{k+1} = x_k - \frac{1}{L}G_M^C f(x_k)$
$\qquad  k \leftarrow k+1$
\end{lstlisting}


\begin{property}
For any $M>0$, we have $x^*$ stationary iff $G_M^C f(x^*)=0$
\end{property}

\begin{property}
Given $f \in C^{1,1}_L$, and letting $x^+ = x - \frac{1}{L}G^C_L f(x)$ be the next step, we have
\[
    f(x) - f(x^+) \ge \frac{\left\|G_L^C f(x)\right\|^2}{2L}
\]
\end{property}


\begin{theorem} Using those properties, we obtain that for $f \in C_L^{1,1}$, the projected gradient method gives
\begin{equation*}
\min_{0\leq i \leq N} \left\|G_L^C f(x_i)\right\| \leq \sqrt{\frac{2(f(x_0) - f(x^*))}{L(N+1)}}
\end{equation*}
\end{theorem}

We have a stronger result in the case of a convex $f$, as stated by the following theorem.
\begin{theorem}
Let $f \in F_L^{1,1}$. For any iterate $x_N$ and any $x$,
\begin{equation*}
f(x_N) - f(x^*) \leq \frac{L\left\|x_0 - x^*\right\|^2}{2N}
\end{equation*}
\end{theorem}


The projected gradient method is quite slow because it is in $\mathcal{O}(\frac{1}{N})$ but also because projection can be complicated. Indeed, projection can be hard to compute if $C$ is too complex.

\begin{example}
\begin{leftbar}
If $C=\{x\ge0\}$ this is easy because $[P_C(x)]_i=\left\lbrace
\begin{array}{ll}
x_i & \mbox{if $x_i \ge 0$}\\
0 & \mbox{if $x_i < 0$}
\end{array}
\right.$
\end{leftbar}
\end{example}

\begin{example}
\begin{leftbar}
If $C=\{x|Ax=b\}$ is a subspace, this is expensieve. We have indeed to solve a linear system, which is $\mathcal{O}(n^3)$.
\end{leftbar}
\end{example}

\begin{example}
\begin{leftbar}
If $C=\{x|Ax\leq b\}$ is a polyhedron, this is even more expensive : the projection problem has to be written as a minimization of the distance (quadratic programming). We could also use a theorem that separates the problem on each facet of the polytope.
\end{leftbar}
\end{example}


\subsection{Acceleration gradient [Nesterov 1983]}


\begin{lstlisting}[mathescape,caption=Acceleration gradient]
Given $f \in C^{1,1}_L$, $x_0$, $L$, $k=0$ and $x_{-1}=x_0$
Repeat 
$\qquad y_k = x_k + \beta_k (x_k - x_{k-1})$
$\qquad x_{k+1} = P_C[y_k - \frac{1}{L}\nabla f(y_k)]$
$\qquad k \leftarrow k+1$
\end{lstlisting}

The first step is called an extrapolation step whereas the second is called a gradient step. Note that $y_k$ is not always in $c$. The idea is to recycle the previous work because $(x_k - x_{k-1})$ will be close to the gradient.

\begin{theorem}
For $\beta_k=\frac{k-1}{k+2}$, we have
\begin{equation*}
f(x_N) -  f(x^*) \leq \frac{2L\left\|x_0 - x^*\right\|^2}{(N+1)^2}
\end{equation*}
\end{theorem}

It can be proved that this rate of $\frac{1}{N^2}$ is not improvable. Accelerated gradient is thus a sublinear method (slower than linear). While the gradient method is really robust, the accelerated method is extremely sensitive. Note that $\beta_k$ goes to 1 as $k$ goes to infinity. An example is the Huber function : on the linear part, the steps increase in a quadratic way.\\

\begin{definition}
Given $\mu>0$, f is $\mu$-\textbf{strongly convex} iff
\begin{equation*}
f(\lambda x + (1-\lambda)y)\leq \lambda f(x) + (1-\lambda)f(y) - \frac{\mu}{2}\lambda (1-\lambda)\left\|x - y\right\|^2
\end{equation*}
\end{definition}

A strongly-convex function can be viewed as a function that has no flat part.

\begin{proposition}
If $f \in C^2$ then f is $\mu$-strongly convex iff for all $x$ 
\[
    \lambda_{min}(\nabla^2f(x))\ge \mu
\]
\end{proposition}


\begin{lemma}
For any strongly convex function, we have
\begin{equation*}
\left\|x - x^*\right\|^2 \leq \frac{2}{\mu}[f(x) - f(x^*)]
\end{equation*}
That means that if I decrease the error on the function value, automatically I decrease the distance to the solution. Using previous results, we obtain :
\begin{equation*}
\left\|x_N - x^*\right\|^2 \leq \frac{2}{\mu}\frac{L\left\|x_0 - x^*\right\|^2}{2N} = \frac{L}{\mu}\frac{\left\|x_0 - x^*\right\|^2}{N}
\end{equation*}
\end{lemma}

$\frac{L}{\mu}$ is called the condition number.


\textbf{Remark:} The accelerated gradient method gives better results if we restart the method after a certain number of iteration. For example, if the squared norm of the error is divided by 5 after $N = 10$ iterations, it will be divided by 10 after $N = 20$ iterations. But if we restart the method after 10 iterations and make 10 new iterations, the square norm of the error will be divided by 25 although we made a total of 20 iterations in both cases. A result that is not proven here is that the optimal number of iterations after which the method should be restarted is $\frac{L}{\mu}\text{e}$. In this case, the method is linearly convergent and we have :
\[
    \left\|x_N - x^*\right\|^2 \leq \left(1-\frac{1}{\text{e}\frac{L}{\mu}}\right)\left\|x_0 - x^*\right\|^2
\]


\section{Note on linear and sublinear convergence}

\textbf{Linear convergence (to zero) :} $1, \rho, \rho ^2, \rho ^3, \rho ^4, \rho ^5, ... $ with $\rho < 1$. If we restrict the class of functions to \textbf{strongly convex}, we can get linear convergence.\\


 For methods with sublinear convergence, the value of the error firstly downs very quickly but after some value, the decreases becomes really slow. For methods with linear convergence, the decrease of the error is, at the beginning, slowler than sublinear methods. However, after the value where the decrease for sublinear methods becomes slow, the decrease for linear method remains constant on a logarithmic scale. \\ \\ Thus, the choice of the method depends on the application. If one needs a lot of accuracy, it is preferable to use linear methods but if high accuracy is not that much important (i.e. we only look for one digit), one then should use sublinear methods. Some graphs to illustrated these results are given on Figure \ref{fig:linear_convergence}.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{images/L6-Linear_convergence.pdf} 
\caption{Illustration of linear and sublinear convergence.}
\label{fig:linear_convergence}
\end{figure}

%\end{document}
